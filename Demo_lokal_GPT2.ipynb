{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPdla8VqVz7K/3E2AqSs6CG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elangbijak4/AI-General/blob/main/Demo_lokal_GPT2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
      ],
      "metadata": {
        "id": "QVZsT5sTjz4K"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqsCvw5HhPbZ",
        "outputId": "a60a0e4d-82be-4b07-8b5d-447f68d33d7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `2.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the history of human ills, and the history of the human condition.\n",
            "\n",
            "The first step in the process of understanding the human condition is to understand the human condition. The human condition is the condition of the human being. The human condition is the condition of the human being. The human condition is the condition of the human being. The human condition is the condition of the human being. The human condition is the condition of the human being. The human condition is the condition of the human being\n"
          ]
        }
      ],
      "source": [
        "# Load pre-trained model tokenizer and model\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "# Move model to GPU\n",
        "model = model.cuda()\n",
        "\n",
        "# Encode input prompt\n",
        "input_text = \"the history of human \"\n",
        "encoded_input = tokenizer.encode(input_text, return_tensors='pt')\n",
        "encoded_input = encoded_input.cuda()\n",
        "\n",
        "# Generate text\n",
        "output_sequences = model.generate(\n",
        "    input_ids=encoded_input,\n",
        "    max_length=100,  # Adjust the max length of the generated text\n",
        "    temperature=2.0,  # Control the randomness of predictions\n",
        "    num_return_sequences=1,  # Number of output sequences\n",
        ")\n",
        "\n",
        "# Decode and print the generated text\n",
        "generated_text = tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
        "print(generated_text)"
      ]
    }
  ]
}